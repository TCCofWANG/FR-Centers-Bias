import torch

import os
import yaml
import json
from .model_tool import get_item
from .dist import is_master_process


class SaveManager(object):
    def __init__(self, output_dir, model_name, metric_name, ckpt_save_freq=1, compare_type='gt', last_metric=None):
        self.output_dir = output_dir
        self.last_metric = last_metric
        self.model_name = model_name
        self.metric_name = metric_name
        self.ckpt_save_freq = ckpt_save_freq
        self.compare_type = compare_type

        assert ckpt_save_freq > 0
        self.ckpt_save_cnt = ckpt_save_freq - 1

        if compare_type == 'gt':
            if last_metric is None:
                self.last_metric = float('-inf')
        elif compare_type == 'lt':
            if last_metric is None:
                self.last_metric = float('inf')
        else:
            raise ValueError('compare type error!')

        assert len(self.metric_name.split('_')) <= 1, 'metric_name should not use _ to split words'

        self.current_best_models = [f for f in os.listdir(
            self.output_dir) if f.startswith('best')]

    def check(self):
        return not is_master_process() or not self.output_dir

    def _compare(self, src, dst):
        if self.compare_type == 'gt':
            return src > dst
        elif self.compare_type == 'lt':
            return src < dst

    def save_epoch_log(self, run_type: str, **kwargs):
        if self.check():
            return

        for k, v in kwargs.items():
            kwargs[k] = get_item(v)

        with open(os.path.join(self.output_dir, '{}_epoch_log.txt'.format(run_type)), 'a+') as f:
            f.write(json.dumps(kwargs) + '\n')

    def save_step_log(self, run_type: str, **kwargs):
        if self.check():
            return

        for k, v in kwargs.items():
            kwargs[k] = get_item(v)

        with open(os.path.join(self.output_dir, '{}_step_log.txt'.format(run_type)), 'a+') as f:
            f.write(json.dumps(kwargs) + '\n')

    def save_hparam(self, args):
        # args: args from argparse return
        if self.check():
            return

        value2save = {k: v for k, v in vars(args).items() if not k.startswith('__') and not k.endswith('__')}
        with open(os.path.join(self.output_dir, 'hparam.yaml'), 'a+') as f:
            f.write(yaml.dump(value2save))

    @staticmethod
    def parse_metric(file_name, metric_name):
        _tmp_str = str(metric_name) + '_'
        idx = file_name.find(_tmp_str) + len(_tmp_str)
        value = float(file_name[idx:file_name.find('_', idx)])
        return value

    def save_ckpt(self, checkpoint_stats, backbone_stats, epoch, cur_metric, point_round=5):
        # save checkpoint
        if self.check():
            return

        checkpoint_name = '{}_epoch_{}_{}_{}_checkpoint.ckpt'.format(
            self.model_name, epoch, self.metric_name, round(cur_metric, point_round))
        checkpoint_path = os.path.join(self.output_dir, checkpoint_name)

        self.ckpt_save_cnt += 1
        if checkpoint_stats is not None:
            latest_ckpts = [f for f in os.listdir(self.output_dir) if f.startswith('latest')]
            if latest_ckpts:
                latest_ckpt_name = latest_ckpts[0]
                latest_epoch = self.parse_metric(latest_ckpt_name, 'latest')
                if epoch > latest_epoch:
                    os.remove(os.path.join(self.output_dir, latest_ckpt_name))
            latest_ckpt_path = os.path.join(self.output_dir, 'latest_{}_checkpoint.ckpt'.format(epoch))
            torch.save(checkpoint_stats, latest_ckpt_path)

            if self.ckpt_save_cnt >= self.ckpt_save_freq:
                checkpoint_stats['save_cnt'] = self.ckpt_save_cnt
                self.ckpt_save_cnt = 0
                torch.save(checkpoint_stats, checkpoint_path)

        # save the best
        if self._compare(cur_metric, self.last_metric):
            self.last_metric = cur_metric
            get_best = True

            current_best_models = [f for f in os.listdir(self.output_dir) if f.startswith('_best')]

            assert len(current_best_models) <= 1

            if len(current_best_models) == 1:
                current_best_model_name = current_best_models[0]

                last_best_metric = self.parse_metric(current_best_model_name, self.metric_name)

                if self._compare(last_best_metric, self.last_metric):
                    self.last_metric = last_best_metric
                    get_best = False
                else:
                    current_best_model_path = os.path.join(self.output_dir, current_best_model_name)
                    os.remove(current_best_model_path)

            if get_best and backbone_stats is not None:
                best_model_name = checkpoint_name.replace('checkpoint', 'model')
                best_model_name = '_best_' + best_model_name
                best_model_path = os.path.join(self.output_dir, best_model_name)
                torch.save(backbone_stats, best_model_path)

                simple_name = 'best_{}.pth'.format(self.model_name)
                simple_path = os.path.join(self.output_dir, simple_name)
                torch.save(backbone_stats, simple_path)


def ddp_module_replace(param_ckpt):
    return {k.replace('module.', ''): v.cpu() for k, v in param_ckpt.items()}
